{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stack_classifer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhHECXTjFN5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "12e51665-dd3a-4604-e467-10aca722d10e"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import time\n",
        "import pathlib\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNJmCjkmFPg5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c30b16a0-7519-4acc-f6b3-2fedfeea5307"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "quzkzXhWFPpc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1796c6a5-01c8-49d3-e908-ada08a28ac21"
      },
      "source": [
        "data_root = pathlib.Path('/content/drive/My Drive/Smash_Bros_Master/_stacks_mini/')\n",
        "label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
        "print(label_names)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bg', 'falcon', 'fox', 'pikachu']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AwlKKbwFPmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE = 32\n",
        "# AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 32\n",
        "# SHUFFLE_SIZE = 1000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "No2KWGY8_-UJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "949e3310-0efa-444a-b5f7-e7d2eaa2bc39"
      },
      "source": [
        "from tensorflow.keras import layers, models\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(8, (3, 3), activation='relu', \n",
        "                        input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Conv2D(16, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(rate=0.1))\n",
        "model.add(layers.Dense(4, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_8 (Conv2D)            (None, 30, 30, 8)         224       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 15, 15, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 13, 13, 16)        1168      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 6, 6, 16)          0         \n",
            "_________________________________________________________________\n",
            "flatten_4 (Flatten)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 576)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 4)                 2308      \n",
            "=================================================================\n",
            "Total params: 3,700\n",
            "Trainable params: 3,700\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSphYnfqGAvY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "4d1bf02c-a2e3-4323-8c18-5e81172398e3"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                   zoom_range=0.9,\n",
        "                                   width_shift_range=0.1,\n",
        "                                   height_shift_range=0.1)\n",
        "\n",
        "# train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        data_root,\n",
        "        target_size=(IMG_SIZE, IMG_SIZE),\n",
        "        batch_size=32,\n",
        "        class_mode='sparse')\n",
        "\n",
        "for data_batch, labels_batch in train_generator:\n",
        "    print('data batch shape:', data_batch.shape)\n",
        "    print('labels batch shape:', labels_batch.shape)\n",
        "    break\n",
        "    \n",
        "history = model.fit_generator(\n",
        "      train_generator,\n",
        "      steps_per_epoch=230,\n",
        "      epochs=5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 7357 images belonging to 4 classes.\n",
            "data batch shape: (32, 32, 32, 3)\n",
            "labels batch shape: (32,)\n",
            "Epoch 1/5\n",
            "230/230 [==============================] - 14s 62ms/step - loss: 0.4612 - accuracy: 0.8483\n",
            "Epoch 2/5\n",
            "230/230 [==============================] - 14s 62ms/step - loss: 0.1258 - accuracy: 0.9602\n",
            "Epoch 3/5\n",
            "230/230 [==============================] - 15s 63ms/step - loss: 0.0913 - accuracy: 0.9731\n",
            "Epoch 4/5\n",
            "230/230 [==============================] - 14s 62ms/step - loss: 0.0747 - accuracy: 0.9750\n",
            "Epoch 5/5\n",
            "230/230 [==============================] - 14s 63ms/step - loss: 0.0664 - accuracy: 0.9796\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJoTeRWCIelw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "model.save(\"/content/stack_model_9.h5\")\n",
        "files.download(\"/content/stack_model_9.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5L0DwOqST00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataSetGenerator():\n",
        "    def __init__(self, IMG_HEIGHT, IMG_WIDTH):\n",
        "        self.IMG_HEIGHT = IMG_HEIGHT\n",
        "        self.IMG_WIDTH = IMG_WIDTH\n",
        "\n",
        "    @tf.function\n",
        "    def load_and_preprocess_image(self, path):\n",
        "        img = tf.io.read_file(path)\n",
        "        img = tf.image.decode_jpeg(img, channels=3)\n",
        "        img = tf.image.resize(img, [self.IMG_HEIGHT, self.IMG_WIDTH])\n",
        "        img /= 255.0  # normalize to [0,1] range\n",
        "        return img\n",
        "\n",
        "    def prepare_for_training(self, ds, cache=True):\n",
        "        # This is a small dataset, only load it once, and keep it in memory.\n",
        "        # use `.cache(filename)` to cache preprocessing work for datasets that don't\n",
        "        # fit in memory.\n",
        "        if cache:\n",
        "            if isinstance(cache, str):\n",
        "                ds = ds.cache(cache)\n",
        "        else:\n",
        "          ds = ds.cache()\n",
        "        \n",
        "        ds = ds.shuffle(buffer_size=self.SHUFFLE_SIZE)\n",
        "\n",
        "        ds = ds.batch(self.BATCH_SIZE)\n",
        "\n",
        "        # `prefetch` lets the dataset fetch batches in the background while the model\n",
        "        # is training.\n",
        "        ds = ds.prefetch(buffer_size=self.AUTOTUNE)\n",
        "\n",
        "        return ds\n",
        "\n",
        "    def labeled_dataset(self, image_paths, labels):\n",
        "        # a dataset that returns image paths\n",
        "        path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "        # for n, img_path in enumerate(path_ds.take(4)):\n",
        "        #     print(n, img_path)\n",
        "\n",
        "        # a dataset that returns images (loaded off disk, decoded, and preprocessed)\n",
        "        image_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=self.AUTOTUNE)\n",
        "        # for n, image in enumerate(image_ds.take(4)):\n",
        "        #     print(n, image.shape)\n",
        "\n",
        "        # a dataset that returns labels\n",
        "        label_ds = tf.data.Dataset.from_tensor_slices(tf.cast(labels, tf.int64))\n",
        "        # for label in label_ds.take(4):\n",
        "        #     print(self.label_names[label.numpy()])\n",
        "\n",
        "        # a dataset that returns images and labels\n",
        "        image_label_ds = tf.data.Dataset.zip((image_ds, label_ds))\n",
        "        # for img, label in image_label_ds.take(2):\n",
        "        #     print(img.shape, self.label_names[label.numpy()])\n",
        "\n",
        "        return image_label_ds\n",
        "\n",
        "    def get_final_dataset(self, data_root, SHUFFLE_SIZE, BATCH_SIZE):\n",
        "        self.SHUFFLE_SIZE =SHUFFLE_SIZE\n",
        "        self.BATCH_SIZE = BATCH_SIZE\n",
        "        self.AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "        # a dataset that returns image paths\n",
        "        data_root = pathlib.Path(data_root)\n",
        "        all_image_paths = list(data_root.glob('*/*'))\n",
        "        all_image_paths = [str(path) for path in all_image_paths if str(path).lower().endswith(\"png\")]\n",
        "        random.shuffle(all_image_paths)\n",
        "        print('There are %d images in this dataset.' % len(all_image_paths))\n",
        "\n",
        "        self.label_names = sorted(item.name for item in data_root.glob('*/') if item.is_dir())\n",
        "        print('All categories in the dataset:', self.label_names)\n",
        "        label_to_index = dict((name, index) for index, name in enumerate(self.label_names))\n",
        "        all_labels = [label_to_index[pathlib.Path(path).parent.name]\n",
        "                        for path in all_image_paths]\n",
        "\n",
        "        # separate dataset\n",
        "        train_paths, test_paths, train_labels, test_labels = train_test_split(all_image_paths, all_labels)\n",
        "\n",
        "        train_ds = self.labeled_dataset(train_paths, train_labels)\n",
        "        train_ds = self.prepare_for_training(train_ds)\n",
        "\n",
        "        test_ds = self.labeled_dataset(test_paths, test_labels)\n",
        "        test_ds = self.prepare_for_training(test_ds)\n",
        "\n",
        "        return train_ds, test_ds\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lscdgYUP_eXM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "0d3b9bec-0891-4201-8c85-f1997cc4fd4d"
      },
      "source": [
        "IMG_SIZE = 32\n",
        "BATCH_SIZE = 32\n",
        "SHUFFLE_SIZE = 20000\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "data_root = pathlib.Path('/content/drive/My Drive/Smash_Bros_Master/_stacks_mini/')\n",
        "dsg = DataSetGenerator(IMG_SIZE,IMG_SIZE)\n",
        "train_ds, test_ds = dsg.get_final_dataset(data_root, SHUFFLE_SIZE, BATCH_SIZE)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 7359 images in this dataset.\n",
            "All categories in the dataset: ['bg', 'falcon', 'fox', 'pikachu']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNcLgzFGAx4O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "b185d82d-a736-4ba2-ca3c-e1786014cc26"
      },
      "source": [
        "from tensorflow.keras import layers, models\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', \n",
        "                        input_shape=(IMG_SIZE, IMG_SIZE, 3)))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D())\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dropout(rate=0.2))\n",
        "model.add(layers.Dense(4, activation='softmax'))\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_2 (Conv2D)            (None, 30, 30, 32)        896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 15, 15, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 2304)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 4)                 9220      \n",
            "=================================================================\n",
            "Total params: 28,612\n",
            "Trainable params: 28,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajFlq2RIGYJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=2, mode='auto')\n",
        "history = model.fit(train_ds, validation_data=test_ds, epochs=10, callbacks=[es_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}